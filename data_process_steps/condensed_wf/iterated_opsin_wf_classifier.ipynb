{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846ccd62",
   "metadata": {},
   "source": [
    "# <font color=green>deepBreaks Applications</font>\n",
    "## Predicting the Phenotype of an Opsin From its Amino Acid Sequence...\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All neccessary packages to import for data process steps.\n",
    "import mysql\n",
    "#install mysql-connector-python // NOT mysql-connector\n",
    "import mysql.connector\n",
    "import argparse\n",
    "import re\n",
    "# importing deepBreaks libraries \n",
    "from deepBreaks import preprocessing as prp\n",
    "from deepBreaks import visualization as viz\n",
    "from deepBreaks import models as ml\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "from pycaret.classification import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f084f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0\n",
    "for nums in range(0,100):\n",
    "    wds = open(\"wds_ni_fmt.fasta\", \"r\").readlines()\n",
    "    meta = open(\"class_ni_meta.tsv\", \"r\",)\n",
    "    tsv = csv.reader(meta, delimiter=\"\\t\")\n",
    "    wds_meta = open(\"wds_ni_meta.tsv\", \"r\")\n",
    "    wds_tsv = csv.reader(wds_meta, delimiter=\"\\t\")\n",
    "    data_columns = [\"Seq_Name\",\"Known_Class\",\"Predicted_Class\",\"Lambda_Max\",\"Score\",\"Correct\",\"Test\"]\n",
    "    results_df = pd.DataFrame(columns=data_columns)  \n",
    "    td = open(\"trainingdata.fasta\", \"w\")\n",
    "    md = open(\"meta_data.tsv\", \"w\")\n",
    "    ud = open(\"unseendata.fasta\", \"w\")\n",
    "    x = 0\n",
    "    k = 0\n",
    "    m = 0\n",
    "\n",
    "    #gives us X random numbers between 1-Y\n",
    "    randomlist = random.sample(range(1,686), 10)\n",
    "\n",
    "    for entry in tsv:\n",
    "        print(entry)\n",
    "        if m == 0:\n",
    "            m+=1\n",
    "        else:    \n",
    "            #read the index entry for row -> if entry is in ranlist then pass write to the dataframe AND to the unseen data file\n",
    "            #else you write to the training data and meta file \n",
    "            #going to need another if statement that 'passes' if k = 0 or 1, then adds 1\n",
    "            #Actual sequences start on line '2' because of column names and bovine... \n",
    "            if entry[0] == \"Bovine\":\n",
    "                data = wds[x]\n",
    "                td.write(data)\n",
    "                x+=1\n",
    "                data = wds[x]\n",
    "                td.write(data)\n",
    "                x+=1\n",
    "                if k == 0:\n",
    "                    md.write(\"\\tOpsin_Class\\n\")\n",
    "                    k+=1\n",
    "                md.write(entry[0]+\"\\t\"+entry[1]+\"\\n\")\n",
    "                \n",
    "            else:\n",
    "                seq_name = int(entry[0].replace(\"S\",\"\"))\n",
    "                opc = int(entry[1])\n",
    "\n",
    "                if seq_name in randomlist:     \n",
    "                    for max in wds_tsv:\n",
    "                        if max[0] == entry[0]:\n",
    "                            lambda_max = max[1]\n",
    "                            break\n",
    "                        else:\n",
    "                            pass\n",
    "                    #write to dataframe\n",
    "                    results_df = results_df.append({\"Seq_Name\":entry[0],\"Known_Class\":entry[1],\"Lambda_Max\":lambda_max}, ignore_index = True)\n",
    "                    print(results_df)\n",
    "                    #write to the unseen data file\n",
    "                    data = wds[x].replace(\"\\n\",\"\")+\"_OPC\"+entry[1]+\"\\n\"\n",
    "                    x+=1\n",
    "                    data+= wds[x]\n",
    "                    ud.write(data)\n",
    "                    x+=1\n",
    "                    #call the whole dataset data file\n",
    "                    #search for \">S{k} in file and take that line [add an '_opcX to it] + following line and copy it over to the unseen data file\"\n",
    "                    #write to dictionary?\n",
    "                \n",
    "                else:\n",
    "                    #write to the training data file\n",
    "                    data = wds[x]\n",
    "                    td.write(data)\n",
    "                    x+=1\n",
    "                    data = wds[x]\n",
    "                    td.write(data)\n",
    "                    x+=1\n",
    "                    if k == 0:\n",
    "                        md.write(\"\\tOpsin_Class\\n\")\n",
    "                        k+=1\n",
    "                    md.write(entry[0]+\"\\t\"+entry[1]+\"\\n\")\n",
    "        \n",
    "    #print(results_df)\n",
    "    td.close() \n",
    "    md.close()\n",
    "    ud.close()\n",
    "\n",
    "    # defining user params, file pathes, analysis type\n",
    "\n",
    "    # path to sequences\n",
    "    seqFileName = 'trainingdata.fasta' \n",
    "    meta_data = 'meta_data.tsv'\n",
    "    # name of the phenotype\n",
    "    mt = 'Opsin_Class'\n",
    "\n",
    "    # type of the sequences\n",
    "    seq_type = 'amino-acid'\n",
    "    # type of the analysis if it is a classification model, then we put cl instead of reg\n",
    "    anaType = 'cl'\n",
    "    sampleFrac=1\n",
    "\n",
    "    # making a unique directory for saving the reports of the analysis\n",
    "    print('direcory preparation')\n",
    "    dt_label = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    seqFile = seqFileName.split('.')[0]\n",
    "    report_dir = str(seqFile +'_' + mt + '_' + dt_label)\n",
    "    os.makedirs(report_dir)\n",
    "\n",
    "    #%%time\n",
    "    # importing sequences data\n",
    "    print('reading fasta file')\n",
    "    df = prp.read_data(seqFileName, seq_type = seq_type, is_main=True)\n",
    "    metaData = prp.read_data(meta_data, is_main=False)\n",
    "    positions = df.shape[1]\n",
    "    print('Done')\n",
    "    print('Shape of data is: ', df.shape)\n",
    "\n",
    "  \n",
    "    # selecting only the classes with enough number of samples\n",
    "    df = prp.balanced_classes(dat=df, meta_dat=metaData, feature=mt)\n",
    "\n",
    "    df.head()\n",
    "\n",
    "    print('metadata looks like this:')\n",
    "    metaData.head()\n",
    "\n",
    "    #%%time\n",
    "    # taking care of missing data\n",
    "    print('Shape of data before missing/constant care: ', df.shape)\n",
    "    df = prp.missing_constant_care(df)\n",
    "    print('Shape of data after missing/constant care: ', df.shape)\n",
    "    #%%time\n",
    "\n",
    "    # taking care of ultra-rare cases\n",
    "    print('Shape of data before imbalanced care: ', df.shape)\n",
    "    df = prp.imb_care(dat=df)\n",
    "    print('Shape of data after imbalanced care: ', df.shape)\n",
    "\n",
    "    #%%time\n",
    "    # Use statistical tests to drop redundant features.\n",
    "    print('number of columns of main data befor: ', df.shape[1])\n",
    "    df_cleaned = prp.redundant_drop(dat=df, meta_dat=metaData,\n",
    "                            feature=mt, model_type=anaType,\n",
    "                            threshold=0.25,\n",
    "                            report_dir=report_dir)\n",
    "    print('number of columns of main data after: ', df_cleaned.shape[1])\n",
    "\n",
    "    #%%time\n",
    "    print('one-hot encoding the dataset')\n",
    "    df_cleaned = prp.get_dummies(dat=df_cleaned, drop_first=True)\n",
    "\n",
    "\n",
    "    #%%time\n",
    "    print('calculating the distance matrix')\n",
    "    cr = prp.distance_calc(dat=df_cleaned,\n",
    "                        dist_method='correlation',\n",
    "                        report_dir=report_dir)\n",
    "    print(cr.shape)\n",
    "\n",
    "    print('The distance matrix looks like this.\\n The values are between 0 (exact the same) and 1 (non-related).')\n",
    "    cr.head()\n",
    "\n",
    "    #%%time\n",
    "    print('finding colinear groups')\n",
    "\n",
    "    dc_df = prp.db_grouped(dat = cr,\n",
    "                        report_dir=report_dir,\n",
    "                        threshold=.3,\n",
    "                        needs_pivot=False)\n",
    "    print('The result of the last step is a dataframe with two columns,\\\n",
    "    1)feature and 2)group.\\nif there are no groups, it will be an empty dataframe')\n",
    "    dc_df.head()\n",
    "\n",
    "    #%%time\n",
    "    print('grouping features')\n",
    "    dc = prp.group_features(dat=df_cleaned,\n",
    "                            group_dat=dc_df,\n",
    "                            report_dir=report_dir)\n",
    "    #file2 = open('dc_cols.txt', 'a+')\n",
    "    #training = str(dc)\n",
    "    #file2.write(training)\n",
    "    #file2.close()  \n",
    "                        \n",
    "    print('dropping correlated features')\n",
    "    print('Shape of data before collinearity care: ', df_cleaned.shape)\n",
    "    df_cleaned = prp.cor_remove(df_cleaned, dc)\n",
    "    print('Shape of data after collinearity care: ', df_cleaned.shape)\n",
    "    file2 = open('train_cols.txt', 'w')\n",
    "    training = (df_cleaned.to_string())\n",
    "    file2.write(training)\n",
    "    file2.close()\n",
    "\n",
    "    #USE FOR TRAIN COLS! It's the correct format. Import into excel. \n",
    "\n",
    "    training_col_list = df_cleaned.columns.tolist()\n",
    "    #merge with meta data\n",
    "    df = df.merge(metaData[mt], left_index=True, right_index=True)\n",
    "    df_cleaned = df_cleaned.merge(metaData[mt], left_index=True, right_index=True)\n",
    "\n",
    "    #file2 = open('testing_cols.txt', 'a+')\n",
    "    #raw_training = df.to_string()\n",
    "    #training = df_cleaned.to_string()\n",
    "    #file2.write(raw_training)\n",
    "    #file2.write(training)\n",
    "    #file2.close()\n",
    "\n",
    "    #%%time\n",
    "    models_to_select = 3 # number of top models to select\n",
    "    top_models, train_cols, model_names = ml.fit_models(dat = df_cleaned,\n",
    "                                                        meta_var=mt,\n",
    "                                                        model_type=anaType, \n",
    "                                                        models_to_select=models_to_select,\n",
    "                                                        report_dir=report_dir)\n",
    "\n",
    "    for i in range(models_to_select):\n",
    "        # calculate the featre importances\n",
    "        imp = ml.fimp_single(trained_model=top_models[i], model_name=model_names[i],\n",
    "                            train_cols=train_cols, grouped_features=dc,\n",
    "                            n_positions=positions, report_dir=report_dir)\n",
    "        # visualize the lollipop plot for features based on each model\n",
    "        viz.dp_plot(dat = imp, model_name= model_names[i],imp_col='standard_value', report_dir=report_dir)\n",
    "        \n",
    "        # visualize the boxplots for features based on each model\n",
    "        viz.plot_imp_model(dat=df, trained_model=top_models[i],\n",
    "                        model_name=model_names[i],\n",
    "                        train_cols=train_cols, grouped_features=dc,\n",
    "                        meta_var=mt, n_positions=positions,\n",
    "                        model_type=anaType, report_dir=report_dir)\n",
    "\n",
    "        #file2 = open('interpret_cols.txt', 'a+')\n",
    "        #training = str(df.to_string)\n",
    "        #file2.write(training)\n",
    "        #file2.close()\n",
    "        \n",
    "    # merging the results for all the top models\n",
    "    mean_imp = ml.fimp_top_models(trained_models=top_models, model_names=model_names, \n",
    "                                train_cols=train_cols,grouped_features=dc,\n",
    "                                n_positions=positions,report_dir=report_dir)\n",
    "\n",
    "    # lollipop plot for the merged results\n",
    "    viz.dp_plot(dat=mean_imp,\n",
    "                model_name= 'mean',\n",
    "                imp_col='mean_imp', \n",
    "                report_dir=report_dir)\n",
    "                \n",
    "    # visualizing top positions \n",
    "    #viz.plot_imp_all(trained_models=top_models, dat=df, train_cols=train_cols,\n",
    "                    #grouped_features=dc, meta_var=mt, model_type=anaType,\n",
    "                    #n_positions=positions, report_dir=report_dir)\n",
    "\n",
    "    models = pd.read_csv(report_dir+ '/models_summary.csv', index_col=0)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 2.5), dpi=350)\n",
    "    fig = models.iloc[:6,].plot(x=\"Model\", y=[\"Accuracy\", \"AUC\", \"F1\"],\n",
    "                            kind=\"barh\",\n",
    "                            color=['#648FFF', '#DC267F', '#FFB000'],\n",
    "                            ax=ax,\n",
    "                            ylim=(0,1))\n",
    "    ax.legend(bbox_to_anchor=(0.85, -.1), fontsize = 6, ncol=3)\n",
    "    ax.set_title('Model performances', fontsize=10)\n",
    "    ax.set_xlabel('')\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.ylabel('')\n",
    "    plt.xlim(.98,1)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(report_dir + '/model_performances.pdf'),\n",
    "                    bbox_inches='tight')\n",
    "\n",
    "    pos = int(0)\n",
    "    val = int(0)\n",
    "    data = list()\n",
    "    seq_dict = dict()\n",
    "    unseenData = 'unseendata.fasta' \n",
    "    seq_type2 = 'amino-acid'\n",
    "    #Following creates a dataframe of aa sequences and their positions as collumns.\n",
    "    df = prp.read_data(unseenData, seq_type = seq_type2, is_main=True)\n",
    "    df.head()\n",
    "    #Turning sequence names into a list for indexing\n",
    "    seq_name = df.index.to_list()\n",
    "    print(seq_name)\n",
    "    #for every sequence in the list of sequences the following function will occur. \n",
    "    for seq in seq_name:\n",
    "        data = list()\n",
    "        for col in training_col_list:\n",
    "            #take the str of the column name and split at the '_'\n",
    "            split = str(col).split('_')\n",
    "            #declare a temp position variable to index df \n",
    "            position = str(split[0])\n",
    "            print(position)\n",
    "            #declare the aa we will compare the df index to\n",
    "            aa = str(split[1])\n",
    "            print(aa)\n",
    "            count = len(aa)\n",
    "            #index the df column corresponding to the 'position' variable\n",
    "            s = df[position]\n",
    "            #assign value based on whether the aa at some position matches the training_col aa \n",
    "            if (s[seq_name[pos]] == aa):\n",
    "                val = int(1)\n",
    "            \n",
    "            elif (count > 1):\n",
    "                for n in range(count):\n",
    "                    amino = aa[n]\n",
    "                    \n",
    "                    if (s[seq_name[pos]] == amino):\n",
    "                        val = int(1)\n",
    "                        break\n",
    "                    else:\n",
    "                        val = int(0)   \n",
    "\n",
    "            else:\n",
    "                val = int(0)\n",
    "\n",
    "            data.append(val)\n",
    "            #print([type(i) for i in data])\n",
    "            \n",
    "        seq_dict.update({seq_name[pos] : data})\n",
    "        pos+=1\n",
    "    print(seq_dict)\n",
    "    df_for_testing = pd.DataFrame(data = seq_dict.values() , columns = training_col_list, index = seq_dict.keys(), dtype = int)\n",
    "    print(df_for_testing.head())\n",
    "\n",
    "    file3 = open('fmt_data_cols.txt', 'w')\n",
    "    training = str(df_for_testing.to_string())\n",
    "    file3.write(training)\n",
    "    file3.close()\n",
    "\n",
    "    #interact//model for classification\n",
    "    # save pipeline\n",
    "    save_model(top_models[0], 'YOUR_MODEL_NAME')\n",
    "    # load pipeline\n",
    "    YOUR_MODEL_NAME = load_model('YOUR_MODEL_NAME')\n",
    "\n",
    "    #interpret_model(top_models[0])\n",
    " \n",
    "    #save prediction as dataframe\n",
    "    prediction = predict_model(YOUR_MODEL_NAME, data = df_for_testing)\n",
    "    print(prediction)\n",
    "    for i in range(0,10):\n",
    "        class_prediction = prediction.Label[i]\n",
    "        cl = int(class_prediction)\n",
    "        #score = prediction.Score[i]\n",
    "        #call entry[i] from the results dataframe with something like...\n",
    "        #results_df.loc[i] = {\"Predicted_Class\":cp,\"Score\":score}\n",
    "        results_df.Predicted_Class[i] = class_prediction\n",
    "        #results_df.Score[i] = score \n",
    "        known = int(results_df.Known_Class[i])\n",
    "        if cl == known:\n",
    "            results_df.Correct[i] = 1\n",
    "        else:\n",
    "            results_df.Correct[i] = 0\n",
    "        results_df.Test = nums\n",
    "        #add a correct/incorrect? == 0 or 1\n",
    "        \n",
    "    if z == 0:\n",
    "        results_df.to_csv(path_or_buf=\"classifier_test_results.csv\",index = False,mode=\"a\")\n",
    "        z+=1\n",
    "    else: \n",
    "        results_df.to_csv(path_or_buf=\"classifier_test_results.csv\",index = False,mode=\"a\",header = False)\n",
    "\n",
    "    meta.close()\n",
    "    wds_meta.close\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e259f732ee45861d7208248c8f070a3f3dbf4758e9d87e1053bb0299bddc003"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
