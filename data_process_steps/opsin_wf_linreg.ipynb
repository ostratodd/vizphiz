{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2f4de42",
   "metadata": {},
   "source": [
    "# deepBreaks applications\n",
    "## Modeling maximum wavelength absorption based on opsin gene amino-acid sequence  \n",
    "\n",
    "![alt text](../img/lite_mar/LITE_Mar.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All neccessary packages to import for data process steps.\n",
    "import mysql\n",
    "import mysql.connector\n",
    "#install mysql-connector-python // NOT mysql-connector\n",
    "import argparse\n",
    "import re\n",
    "# importing deepBreaks libraries \n",
    "from deepBreaks import preprocessing as prp\n",
    "from deepBreaks import visualization as viz\n",
    "from deepBreaks import models as ml\n",
    "import os\n",
    "import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 0: Data Base Setup - Script 1 - Import heterologous.tsv into mySQL\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  database= \"vizphiz\",\n",
    "  password= \"Geass5566!!\"\n",
    ")\n",
    "\n",
    "#read file for data\n",
    "file1 = open('heterologous.tsv', 'r', encoding=\"utf8\")\n",
    "Lines = file1.readlines()\n",
    "\n",
    "count=0\n",
    "for line in Lines:\n",
    "    columns = line.split(\"\\t\")\n",
    "    print(columns)\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    sql = \"INSERT INTO opsins.heterologous (hetid, genus, species, accession, mutations, lamdamax, error, cellculture, purification, spectrum, sourcetype, refid) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    val = (columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], columns[6], columns[7], columns[8], columns[9], columns[10], columns[11])\n",
    "    print(sql)\n",
    "    print(val)\n",
    "\n",
    "    mycursor.execute(sql, val)\n",
    "\n",
    "    mydb.commit()\n",
    "\n",
    "    print(mycursor.rowcount, \"record inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 0: Data Base Setup - Script 2 - Import opsindb.tsv into mySQL\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  database= \"vizphiz\",\n",
    "  password= \"Geass5566!!\"\n",
    ")\n",
    "\n",
    "#read file for data\n",
    "file1 = open('heterologous.tsv', 'r', encoding=\"utf8\")\n",
    "Lines = file1.readlines()\n",
    "\n",
    "count=0\n",
    "for line in Lines:\n",
    "    columns = line.split(\"\\t\")\n",
    "    print(columns)\n",
    "    mycursor = mydb.cursor()\n",
    "\n",
    "    sql = \"INSERT INTO opsins.heterologous (hetid, genus, species, accession, mutations, lamdamax, error, cellculture, purification, spectrum, sourcetype, refid) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    val = (columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], columns[6], columns[7], columns[8], columns[9], columns[10], columns[11])\n",
    "    print(sql)\n",
    "    print(val)\n",
    "\n",
    "    mycursor.execute(sql, val)\n",
    "\n",
    "    mydb.commit()\n",
    "\n",
    "    print(mycursor.rowcount, \"record inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: Extract Data From Vizphiz\n",
    "#Output = 5 different 'version' // 'splits' of the data. \n",
    "#Take outputs and run through MAFFT before moving on to STEP2\n",
    "#Suggested parameters for 'mafft' alignment are...\n",
    "wd_output = 'wds.txt'\n",
    "sws_output = 'swd.txt'\n",
    "mws_output = 'mwd.txt'\n",
    "rod_output = 'rod.txt'\n",
    "wh_metadata = 'wds_meta.txt'\n",
    "sw_metadata = 'sws_meta.txt'\n",
    "mw_metadata = 'mws_meta.txt'\n",
    "rh_metadata = 'rod_meta.txt'\n",
    "wd_ni_output = 'wds_noinverts.txt'\n",
    "wd_ni_metadata = 'wds_ni_meta.txt'\n",
    "class_ni_metadata = 'class_meta.txt'\n",
    "class_metadata = 'class_ni_meta.txt'\n",
    "m = 0\n",
    "s = 0\n",
    "l = 0\n",
    "r = 0\n",
    "c = 0\n",
    "acc_list = []\n",
    "duped = 0\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  database=\"vizphiz\",\n",
    "  password=\"Geass5566!!\"\n",
    ") \n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "sql = \"select DISTINCT o.genus,o.species,o.genefamily,o.accession,h.lamdamax, o.aa from opsins.opsins o, opsins.heterologous h WHERE (o.accession = h.accession AND o.refid = h.refid); \"\n",
    "mycursor.execute(sql)\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "\n",
    "for x in myresult:       \n",
    "\n",
    "  if (x[4] in range(340, 380)):\n",
    "    opclass = 0\n",
    "  \n",
    "  elif (x[4] in range(380, 420)):\n",
    "    opclass = 1 \n",
    "\n",
    "  elif (x[4] in range(420, 460)):\n",
    "    opclass = 2\n",
    "\n",
    "  elif (x[4] in range(460, 500)):\n",
    "    opclass = 3 \n",
    "\n",
    "  elif (x[4] in range(500, 540)):\n",
    "    opclass = 4  \n",
    "  else:\n",
    "    opclass = 5\n",
    "\n",
    "  #Keep all accessions in a list -- don't print \n",
    "  if (x[3] in acc_list):\n",
    "    #print (\"\\n\\n\\n****Accession Exists\" + x[2] + \"\\n\\n\" )\n",
    "    duped=1\n",
    "  #First 2 of if statements ignore ancestral (pigment named) or mutated genes\n",
    "  if (duped==1) :\n",
    "    duped=0\n",
    "  #do not print if contains the word \"pigment\" which is an ancestral sequence\n",
    "\n",
    "  else :\n",
    "#REG-DATA SECTION    \n",
    "       with open(wd_output, 'a') as f:\n",
    "          if(x[3]) == \"NM_001014890.2\":\n",
    "            f.write(\">Bovine\\nMNGTEGPNFYVPFSNKTGVVRSPFEAPQYYLAEPWQFSMLAAYMFLLIMLGFPINFLTLYVTVQHKKLRTPLNYILLNLAVADLFMVFGGFTTTLYTSLHGYFVFGPTGCNLEGFFATLGGEIALWSLVVLAIERYVVVCKPMSNFRFGENHAIMGVAFTWVMALACAAPPLVGWSRYIPEGMQCSCGIDYYTPHEETNNESFVIYMFVVHFIIPLIVIFFCYGQLVFTVKEAAAQQQESATTQKAEKEVTRMVIIMVIAFLICWLPYAGVAFYIFTHQGSDFGPIFMTIPAFFAKTSAVYNPVIYIMMNKQFRNCMVTTLCCGKNPLGDDEASTTVSKTETSQVAPA\\n\")\n",
    "          else:\n",
    "            m += 1 \n",
    "            #This makes the fasta format file\n",
    "            seq = \">S\" + str(m)\n",
    "            f.write(seq)\n",
    "            seq2 = str('\\n' + x[5] + '\\n')\n",
    "            f.write(seq2)\n",
    "\n",
    "       with open(wd_ni_output, 'a') as f:\n",
    "        p = re.compile('^Rtc|^BRh[0-3]|Pr[A-Z]')\n",
    "        if (p.match(x[2])):\n",
    "          pass\n",
    "        else:\n",
    "          if(x[3]) == \"NM_001014890.2\":\n",
    "            f.write(\">Bovine\\nMNGTEGPNFYVPFSNKTGVVRSPFEAPQYYLAEPWQFSMLAAYMFLLIMLGFPINFLTLYVTVQHKKLRTPLNYILLNLAVADLFMVFGGFTTTLYTSLHGYFVFGPTGCNLEGFFATLGGEIALWSLVVLAIERYVVVCKPMSNFRFGENHAIMGVAFTWVMALACAAPPLVGWSRYIPEGMQCSCGIDYYTPHEETNNESFVIYMFVVHFIIPLIVIFFCYGQLVFTVKEAAAQQQESATTQKAEKEVTRMVIIMVIAFLICWLPYAGVAFYIFTHQGSDFGPIFMTIPAFFAKTSAVYNPVIYIMMNKQFRNCMVTTLCCGKNPLGDDEASTTVSKTETSQVAPA\\n\")\n",
    "          else:\n",
    "            c += 1 \n",
    "            #This makes the fasta format file\n",
    "            seq = \">S\" + str(c)\n",
    "            f.write(seq)\n",
    "            seq2 = str('\\n' + x[5] + '\\n')\n",
    "            f.write(seq2)\n",
    "\n",
    "       with open(sws_output, 'a') as f:\n",
    "        p = re.compile('^SWS|^UVS')\n",
    "        if p.match(x[2]):\n",
    "          s+=1\n",
    "          if s == 1:\n",
    "            f.write(\">Bovine\\nMNGTEGPNFYVPFSNKTGVVRSPFEAPQYYLAEPWQFSMLAAYMFLLIMLGFPINFLTLYVTVQHKKLRTPLNYILLNLAVADLFMVFGGFTTTLYTSLHGYFVFGPTGCNLEGFFATLGGEIALWSLVVLAIERYVVVCKPMSNFRFGENHAIMGVAFTWVMALACAAPPLVGWSRYIPEGMQCSCGIDYYTPHEETNNESFVIYMFVVHFIIPLIVIFFCYGQLVFTVKEAAAQQQESATTQKAEKEVTRMVIIMVIAFLICWLPYAGVAFYIFTHQGSDFGPIFMTIPAFFAKTSAVYNPVIYIMMNKQFRNCMVTTLCCGKNPLGDDEASTTVSKTETSQVAPA\\n\")  \n",
    "        #This makes the fasta format file\n",
    "          seq = \">S\" + str(s)\n",
    "          f.write(seq)\n",
    "          seq2 = str('\\n' + x[5] + '\\n')\n",
    "          f.write(seq2)\n",
    "\n",
    "       with open(mws_output, 'a') as f:\n",
    "        p = re.compile('^MWS|^LWS')\n",
    "        if p.match(x[2]):\n",
    "          l+=1\n",
    "          if l == 1:\n",
    "            f.write(\">Bovine\\nMNGTEGPNFYVPFSNKTGVVRSPFEAPQYYLAEPWQFSMLAAYMFLLIMLGFPINFLTLYVTVQHKKLRTPLNYILLNLAVADLFMVFGGFTTTLYTSLHGYFVFGPTGCNLEGFFATLGGEIALWSLVVLAIERYVVVCKPMSNFRFGENHAIMGVAFTWVMALACAAPPLVGWSRYIPEGMQCSCGIDYYTPHEETNNESFVIYMFVVHFIIPLIVIFFCYGQLVFTVKEAAAQQQESATTQKAEKEVTRMVIIMVIAFLICWLPYAGVAFYIFTHQGSDFGPIFMTIPAFFAKTSAVYNPVIYIMMNKQFRNCMVTTLCCGKNPLGDDEASTTVSKTETSQVAPA\\n\")\n",
    "          #This makes the fasta format file\n",
    "          seq = \">S\" + str(l)\n",
    "          f.write(seq)\n",
    "          seq2 = str('\\n' + x[5] + '\\n')\n",
    "          f.write(seq2)\n",
    "\n",
    "       with open(rod_output, 'a') as f:\n",
    "        p = re.compile('Rh[0-2]|exoRh')\n",
    "        if p.match(x[2]):\n",
    "          if(x[3]) == \"NM_001014890.2\":\n",
    "            f.write(\">Bovine\\nMNGTEGPNFYVPFSNKTGVVRSPFEAPQYYLAEPWQFSMLAAYMFLLIMLGFPINFLTLYVTVQHKKLRTPLNYILLNLAVADLFMVFGGFTTTLYTSLHGYFVFGPTGCNLEGFFATLGGEIALWSLVVLAIERYVVVCKPMSNFRFGENHAIMGVAFTWVMALACAAPPLVGWSRYIPEGMQCSCGIDYYTPHEETNNESFVIYMFVVHFIIPLIVIFFCYGQLVFTVKEAAAQQQESATTQKAEKEVTRMVIIMVIAFLICWLPYAGVAFYIFTHQGSDFGPIFMTIPAFFAKTSAVYNPVIYIMMNKQFRNCMVTTLCCGKNPLGDDEASTTVSKTETSQVAPA\\n\")\n",
    "          else:\n",
    "            r+=1\n",
    "            #This makes the fasta format file\n",
    "            seq = \">S\" + str(r)\n",
    "            f.write(seq)\n",
    "            seq2 = str('\\n' + x[5] + '\\n')\n",
    "            f.write(seq2)\n",
    "\n",
    "#METADATA SECTION\n",
    "       with open(wh_metadata, 'a') as g:\n",
    "        #This makes the metadata formatted for a linear regression model.\n",
    "        if m == 1:\n",
    "          g.write(\"\\tLambda_Max\\tSpecies\\tOpsin_Family\\tAccession\\n\")  \n",
    "        if(x[3]) == \"NM_001014890.2\":\n",
    "          g.write(\"Bovine\\t500\\tBos_tarus\\tRh1\\tNM_001014890\\n\")\n",
    "        else:        \n",
    "          md =  str(\"S\" + str(m) + \"\\t\" + str(x[4]).strip()) + \"\\t\" + str(x[0]).strip().replace(' ','') + \"_\" + str(x[1]).strip().replace(' ','') + \"\\t\" + str(x[2]).strip() + \"\\t\" + x[3].strip() + \"\\n\"\n",
    "          g.write(md)\n",
    "\n",
    "       with open(wd_ni_metadata, 'a') as g:\n",
    "        p = re.compile('^Rtc|^BRh[0-3]|Pr[A-Z]')\n",
    "        if p.match(x[2]):\n",
    "          pass\n",
    "        #This makes the metadata formatted for a linear regression model.\n",
    "        else:\n",
    "          if c == 1:\n",
    "            g.write(\"\\tLambda_Max\\tSpecies\\tOpsin_Family\\tAccession\\n\")\n",
    "          if(x[3]) == \"NM_001014890.2\":\n",
    "            g.write(\"Bovine\\t500\\tBos_tarus\\tRh1\\tNM_001014890\\n\")\n",
    "          else:  \n",
    "            md =  str(\"S\" + str(c) + \"\\t\" + str(x[4]).strip()) + \"\\t\" + str(x[0]).strip().replace(' ','') + \"_\" + str(x[1]).strip().replace(' ','') + \"\\t\" + str(x[2]).strip() + \"\\t\" + x[3].strip() + \"\\n\"\n",
    "            g.write(md)\n",
    "\n",
    "       with open(sw_metadata, 'a') as g:\n",
    "        #This makes the metadata formatted for a linear regression model.\n",
    "        p = re.compile('^SWS|^UVS')\n",
    "        if p.match(x[2]):\n",
    "          if s == 1:\n",
    "            g.write(\"\\tLambda_Max\\tSpecies\\tOpsin_Family\\tAccession\\n\")\n",
    "            g.write(\"Bovine\\t500\\tBos_tarus\\tRh1\\tNM_001014890\\n\")  \n",
    "          md =  str(\"S\" + str(s) + \"\\t\" + str(x[4]).strip()) + \"\\t\" + str(x[0]).strip().replace(' ','') + \"_\" + str(x[1]).strip().replace(' ','') + \"\\t\" + str(x[2]).strip() + \"\\t\" + x[3].strip() + \"\\n\"\n",
    "          g.write(md)\n",
    "\n",
    "       with open(mw_metadata, 'a') as g:\n",
    "        #This makes the metadata formatted for a linear regression model.\n",
    "        p = re.compile('^MWS|^LWS')\n",
    "        if p.match(x[2]):\n",
    "          if l == 1:\n",
    "            g.write(\"\\tLambda_Max\\tSpecies\\tOpsin_Family\\tAccession\\n\")  \n",
    "            g.write(\"Bovine\\t500\\tBos_tarus\\tRh1\\tNM_001014890\\n\")  \n",
    "          md =  str(\"S\" + str(l) + \"\\t\" + str(x[4]).strip()) + \"\\t\" + str(x[0]).strip().replace(' ','') + \"_\" + str(x[1]).strip().replace(' ','') + \"\\t\" + str(x[2]).strip() + \"\\t\" + x[3].strip() + \"\\n\"\n",
    "          g.write(md)\n",
    "\n",
    "       with open(rh_metadata, 'a') as g:\n",
    "        #This makes the metadata formatted for a linear regression model.\n",
    "        p = re.compile('Rh[0-3]|exoRh')\n",
    "        if p.match(x[2]):\n",
    "          if r == 1:\n",
    "            g.write(\"\\tLambda_Max\\tSpecies\\tOpsin_Family\\tAccession\\n\")\n",
    "          if(x[3]) == \"NM_001014890.2\":\n",
    "            g.write(\"Bovine\\t500\\tBos_tarus\\tRh1\\tNM_001014890\\n\")\n",
    "          else:  \n",
    "            md =  str(\"S\" + str(r) + \"\\t\" + str(x[4]).strip()) + \"\\t\" + str(x[0]).strip().replace(' ','') + \"_\" + str(x[1]).strip().replace(' ','') + \"\\t\" + str(x[2]).strip() + \"\\t\" + x[3].strip() + \"\\n\"\n",
    "            g.write(md)\n",
    "\n",
    "       with open(class_metadata, 'a') as h:\n",
    "          #This makes the metadata formatted for a classification model. \n",
    "        if m == 1:\n",
    "          h.write(\"\\tOpsin_Class\\n\")\n",
    "        if(x[3]) == \"NM_001014890.2\":\n",
    "          h.write(\"Bovine\\t\" + str(opclass) + \"\\n\")\n",
    "        else:  \n",
    "          md = \"S\" + str(m) + \"\\t\" + str(opclass) + \"\\n\"\n",
    "          h.write(md)\n",
    "\n",
    "       with open(class_ni_metadata, 'a') as h:\n",
    "        p = re.compile('^Rtc|^BRh[0-3]|Pr[A-Z]')\n",
    "        if p.match(x[2]):\n",
    "          pass\n",
    "          #This makes the metadata formatted for a classification model. \n",
    "        else:\n",
    "          if c == 1:\n",
    "            h.write(\"\\tOpsin_Class\\n\")\n",
    "          if(x[3]) == \"NM_001014890.2\":\n",
    "            h.write(\"Bovine\\t\" + str(opclass) + \"\\n\")\n",
    "          else:    \n",
    "            md = \"S\" + str(c) + \"\\t\" + str(opclass) + \"\\n\"\n",
    "            h.write(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e914c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: Format Alignments for 'deepBreaks'\n",
    "#REMINDER - Take outputs from STEP 1 and run through MAFFT before moving on to STEP2\n",
    "#Suggested parameters for 'mafft' alignment are...\n",
    "#enter list of aligned text files here.\n",
    "inputs = []\n",
    "##enter list of names for desired formatted fasta files here.\n",
    "output = []\n",
    "for files in inputs:\n",
    "    file1 = open(inputs, 'r') \n",
    "    file2 = open(output, 'a+')\n",
    "    lines = file1.readlines()\n",
    "    line_count = len(lines)\n",
    "    tab_entry = \"\"\n",
    "    i = 0\n",
    "    k = 0\n",
    "    m = 0\n",
    "    call = input(\"How Many Lines Per Alignment?: \")\n",
    "    count = int(call)\n",
    "    loop = range(int(count))\n",
    "\n",
    "    for line in lines:\n",
    "\n",
    "        for entries in loop:\n",
    "\n",
    "            line1 = str(lines[k])\n",
    "            line1.replace(\"\\n\",\"\").lstrip\n",
    "            if k == i:\n",
    "                tab_entry += line1 \n",
    "\n",
    "            else:\n",
    "                tab_entry += str(line1).replace(\"\\n\", \"\")\n",
    "                tab_entry.rstrip()\n",
    "            k+=1\n",
    "\n",
    "        file2.write(tab_entry + \"\\n\")\n",
    "            \n",
    "        tab_entry = \"\"\n",
    "        #I want code to read two lines at a time, replace any \">\" with empty sapce and any new lines with a tab instead\n",
    "        i+=count\n",
    "        k = i \n",
    "\n",
    "        if i >= line_count:\n",
    "            break\n",
    "    file1.close()\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae93f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3: deepBreaks - THIS IS A LONG SECTION! STEP 4 doesn't start until Cell 34\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f084f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining user params, file pathes, analysis type\n",
    "\n",
    "# path to sequences\n",
    "seqFileName = 'wds_ni_fmt.fasta' \n",
    "\n",
    "# path to metadata\n",
    "metaDataFileName = 'wds_ni_meta.tsv' \n",
    "\n",
    "# name of the phenotype\n",
    "mt = 'Lambda_Max'\n",
    "\n",
    "# type of the sequences\n",
    "seq_type = 'amino-acid'\n",
    "\n",
    "# type of the analysis if it is a classification model, then we put cl instead of reg\n",
    "anaType = 'reg' \n",
    "sampleFrac=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a unique directory for saving the reports of the analysis\n",
    "print('direcory preparation')\n",
    "dt_label = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "seqFile = seqFileName.split('.')[0]\n",
    "report_dir = str(seqFile +'_' + mt + '_' + dt_label)\n",
    "os.makedirs(report_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0ba03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print('reading meta-data')\n",
    "# importing metadata\n",
    "metaData = prp.read_data(metaDataFileName, seq_type = None, is_main=False)\n",
    "print('metaData:', metaData.shape)\n",
    "\n",
    "# importing sequences data\n",
    "print('reading fasta file')\n",
    "df = prp.read_data(seqFileName, seq_type = seq_type, is_main=True)\n",
    "\n",
    "positions = df.shape[1]\n",
    "print('Done')\n",
    "print('Shape of data is: ', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69979653",
   "metadata": {},
   "source": [
    "**Attention**: metadata and sequences data should have the names as their row names and for each sequence their must be a value in the meta data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a259b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('metadata looks like this:')\n",
    "metaData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff750203",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('sequence data looks like this:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f567b",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "In this step, we do all these steps:\n",
    "1. dropping columns with a number of missing values above a certain threshold  \n",
    "2. dropping zero entropy columns  \n",
    "3. imputing missing values with the mode of that column  \n",
    "4. replacing cases with a frequency below a threshold (default 1.5%) with the mode of that column\n",
    "5. dropping zero entropy columns\n",
    "6. use statistical tests (each position against the phenotype) and drop columns with p-values below a threshold (default 0.25)\n",
    "7. one-hot encode the remaining columns\n",
    "8. calculate the pair-wise distance matrix for all of the columns\n",
    "9. use the distance matrix for DBSCAN and cluster the correlated positions together\n",
    "10. keep only one column (closes to center of each cluster) for each group and drop the rest from the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# taking care of missing data\n",
    "print('Shape of data before missing/constant care: ', df.shape)\n",
    "df = prp.missing_constant_care(df)\n",
    "print('Shape of data after missing/constant care: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f04e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# taking care of ultra-rare cases\n",
    "print('Shape of data before imbalanced care: ', df.shape)\n",
    "df = prp.imb_care(dat=df, imbalance_threshold=0.025)\n",
    "print('Shape of data after imbalanced care: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311179eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# you may want to perform your analysis only on a random sample of the positions.\n",
    "# Here you can have a random sample of your main data set.\n",
    "print('number of columns of main data befor: ', df.shape[1])\n",
    "df = prp.col_sampler(dat=df, sample_frac=sampleFrac)\n",
    "print('number of columns of main data after: ', df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Use statistical tests to drop redundant features.\n",
    "print('number of columns of main data befor: ', df.shape[1])\n",
    "df_cleaned = prp.redundant_drop(dat=df, meta_dat=metaData,\n",
    "                        feature=mt, model_type=anaType,\n",
    "                        threshold=0.25,\n",
    "                        report_dir=report_dir)\n",
    "print('number of columns of main data after: ', df_cleaned.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5047c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('one-hot encoding the dataset')\n",
    "df_cleaned = prp.get_dummies(dat=df_cleaned, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('calculating the distance matrix')\n",
    "cr = prp.distance_calc(dat=df_cleaned,\n",
    "                       dist_method='correlation',\n",
    "                       report_dir=report_dir)\n",
    "print(cr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The distance matrix looks like this.\\n The values are between 0 (exact the same) and 1 (non-related).')\n",
    "cr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('finding colinear groups')\n",
    "dc_df = prp.db_grouped(dat=cr,\n",
    "                       report_dir=report_dir,\n",
    "                       threshold=.25, needs_pivot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The result of the last step is a dataframe with two columns,\\\n",
    "1)feature and 2)group.\\nif there are no groups, it will be an empty dataframe')\n",
    "dc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83866ca9",
   "metadata": {},
   "source": [
    "Then, we pass the above calculated groupes into the *group_feature* function. This function finds the distance of all the group members to the center of the group (median). The result will be a dictionary of columns like this:  \n",
    "\\\n",
    "{\\\n",
    "group1_representativ:[member1, member2,...],  \n",
    "group2_representativ:[member1, member2,...],...  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('grouping features')\n",
    "dc = prp.group_features(dat=df_cleaned,\n",
    "                        group_dat=dc_df,\n",
    "                        report_dir=report_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4acd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dropping correlated features')\n",
    "print('Shape of data before collinearity care: ', df_cleaned.shape)\n",
    "df_cleaned = prp.cor_remove(df_cleaned, dc)\n",
    "print('Shape of data after collinearity care: ', df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with metadata\n",
    "df = df.merge(metaData[mt], left_index=True, right_index=True)\n",
    "df_cleaned = df_cleaned.merge(metaData[mt], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d16f3",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "In this step, we try to fit multiple models to the training dataset and rank them based on their performance. By default, we select the top 3 three models for further analysis.  \n",
    "During this step, deepBreaks creates a CSV file containing all the fitted models with their performance metrics. These metrics are based on an average of 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "models_to_select = 3 # number of top models to select\n",
    "top_models, train_cols, model_names = ml.fit_models(dat = df_cleaned,\n",
    "                                                    meta_var=mt,\n",
    "                                                    model_type=anaType, \n",
    "                                                    models_to_select=models_to_select,\n",
    "                                                    report_dir=report_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b02f283",
   "metadata": {},
   "source": [
    "## Interpretation \n",
    "In this step, we use the training data set, positions, and the top models to report the most discriminative positions in the sequences associated with the phenotype.  \n",
    "we report the feature importances for all top models separately and make a box plot (regression) or stacked bar plot (classification) for the top 4 positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ba576",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(models_to_select):\n",
    "    # calculate the featre importances\n",
    "    imp = ml.fimp_single(trained_model=top_models[i], model_name=model_names[i],\n",
    "                         train_cols=train_cols, grouped_features=dc,\n",
    "                         n_positions=positions, report_dir=report_dir)\n",
    "    # visualize the lollipop plot for features based on each model\n",
    "    viz.dp_plot(dat = imp, model_name= model_names[i],imp_col='standard_value', report_dir=report_dir)\n",
    "    \n",
    "    # visualize the boxplots for features based on each model\n",
    "    viz.plot_imp_model(dat=df, trained_model=top_models[i],\n",
    "                       model_name=model_names[i],\n",
    "                       train_cols=train_cols, grouped_features=dc,\n",
    "                       meta_var=mt, n_positions=positions,\n",
    "                       model_type=anaType, report_dir=report_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the results for all the top models\n",
    "mean_imp = ml.fimp_top_models(trained_models=top_models, model_names=model_names, \n",
    "                              train_cols=train_cols,grouped_features=dc,\n",
    "                              n_positions=positions,report_dir=report_dir)\n",
    "\n",
    "# lollipop plot for the merged results\n",
    "viz.dp_plot(dat=mean_imp,figsize=(7.2, 3),\n",
    "            model_name='mean',\n",
    "            imp_col='mean_imp', \n",
    "            report_dir=report_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791d77d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizing top positions  \n",
    "viz.plot_imp_all(trained_models=top_models, dat=df, train_cols=train_cols,\n",
    "                 grouped_features=dc, meta_var=mt, model_type=anaType,\n",
    "                 n_positions=positions, report_dir=report_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4: Translate Candidate STSs \n",
    "#Optional step IF running the classifier model, but could still be useful.\n",
    "#To find CSTSs, got to the deepBreaks output folder and find the excel sheet 'avg_top_models_feature_importance'\n",
    "#Sort the top model's feature importance column by 'largest-to-smallest' and take x# of those positions.\n",
    "#enter list of candidate STSs below\n",
    "impsites = input(\"Enter list CSTSs: \")\n",
    "#enter desired name of output text file.\n",
    "#eg - If I'm translating sites from a LRM for the rods subset of my dataset I would have the output text file be named...\n",
    "#output = rod_csts.txt\n",
    "output = input(\"Enter Name of Output File: \")\n",
    "seq = input(\"Enter Aligned Bovine Sequence: \")\n",
    "site_list = impsites.split(',')\n",
    "m = 0\n",
    "\n",
    "#take the list of important sites and translate them to the bovine standard equivalent, we do this by taking the site number and subtracting the number of '-' between the start of the sequence and the desired site. \n",
    "for sites in site_list:\n",
    "\n",
    "    k = int(sites)    \n",
    "    gaps = seq[:k].count('-')\n",
    "    #print(\"The number of gaps is \" + str(gaps))\n",
    "    trans_site = k - gaps\n",
    "    #print(f\"For site {k} the bovine equivalent is {trans_site}\") \n",
    "\n",
    "    with open(output, 'a') as f:\n",
    "        if m == 0:\n",
    "                f.write(\"The following sites have been translated to the standard Bovine equivalent...\\nFormat is before translation --> post translation to bovine equivalent.\")   \n",
    "                m+=1 \n",
    "        f.write(f\"Site {k} == {trans_site}\\n\")      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepBreaks_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:42:03) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed5aafab82c1711937cb6f5bcd06445bf48d3ee36304e1e46581089e6ea661fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
